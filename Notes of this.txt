FULL PROJECT EXPLANATION ‚Äì CROP YIELD PREDICTION

Problem You Solved:

Predict agricultural crop yield (like how much rice or wheat will be produced per hectare) using:

Crop name

State name

Crop year

- ALGORITHMS USED
1. üî¢ One-Hot Encoding (Feature Engineering)
‚û§ Why?
Your input data had categorical variables like:

"Crop" (e.g., Rice, Wheat, Sugarcane)

"State_Name" (e.g., Punjab, Kerala)

"Crop_Year" (numerical)

Machine learning models can't handle string/text ‚Äî so you used One-Hot Encoding to convert categories into binary columns:

Crop_Rice	Crop_Wheat	Crop_Sugarcane	State_Name_Punjab	...
1	0	0	1	...

‚û§ Tools Used:
python
Copy
Edit
pd.get_dummies(df[['Crop', 'State_Name']], drop_first=False)
2. üìâ Log Transformation on Target (yield)
‚û§ Why?
Crop yield has huge variation: some are near 0, others in millions.

That creates skewed data and reduces model performance.

So you applied:

python
Copy
Edit
df['yield'] = np.log1p(df['yield'])
This means your model predicts log(yield + 1).
At the end, you convert it back using: np.expm1(prediction) to get original yield.

3. ‚öôÔ∏è Machine Learning Model ‚Äì XGBoost Regressor
‚úÖ What is XGBoost?
XGBoost (Extreme Gradient Boosting) is one of the most powerful ML algorithms used for:

Regression

Classification

Ranking

It builds multiple decision trees and combines them to improve performance.
Each tree corrects the errors made by the previous one (this is boosting).

‚úÖ Why You Chose XGBoost?
Handles both categorical and numeric features

Handles non-linear data

Works well even with sparse one-hot data

Regularization avoids overfitting

Highly accurate for tabular data

‚úÖ How XGBoost Predicts Crop Yield
It starts with a weak tree that makes rough predictions

It calculates the error (difference between actual and predicted yield)

Then it builds another tree to predict the error

This continues for hundreds of trees, gradually reducing the error

Final prediction is the sum of all tree outputs

Your model predicts log(yield + 1), and then you reverse it using np.expm1(...)

üõ†Ô∏è SOLUTION FLOW ‚Äî STEP-BY-STEP
üîπ STEP 1: Data Collection
Dataset had columns: Crop, State_Name, Crop_Year, and yield

üîπ STEP 2: Data Preprocessing
Removed unnecessary columns

Handled missing values (if any)

Applied One-Hot Encoding on categorical features

Applied log1p transformation to normalize yield

üîπ STEP 3: Model Training
python
Copy
Edit
from xgboost import XGBRegressor

model = XGBRegressor()
model.fit(X_train, y_train)
X_train = one-hot encoded crop, state, and year

y_train = log-transformed yield

üîπ STEP 4: Model Saving
python
Copy
Edit
import pickle
pickle.dump(model, open("xgb_crop_yield_model.pkl", "wb"))
pickle.dump(X.columns.tolist(), open("features_list.pkl", "wb"))
üîπ STEP 5: Streamlit Web App
Built a UI where user selects:

State ‚úÖ

Crop ‚úÖ

Year ‚úÖ

App creates a zero-vector of length = number of features, and sets 1 in correct one-hot position.

python
Copy
Edit
input_data = pd.DataFrame([np.zeros(len(feature_list))], columns=feature_list)
input_data["Crop_Wheat"] = 1
input_data["State_Name_Bihar"] = 1
input_data["Crop_Year"] = 2009
Model gives log prediction ‚Üí convert it back

python
Copy
Edit
log_pred = model.predict(input_data)
yield_pred = np.expm1(log_pred)
App shows predicted result using:

python
Copy
Edit
st.success(f"Estimated Yield: {yield_pred:.2f} units")
‚úÖ OUTPUT
Simple, responsive web app built with Streamlit

Real-time prediction of crop yield

Can be used by:

Farmers

Agri researchers

Policymakers

üîö Summary
Component	Explanation
Preprocessing	One-hot encoding + log transform
Model	XGBoost (regression)
Output	log prediction reversed using np.expm1()
Deployment	Streamlit app with dropdown inputs
Accuracy Achieved	~42% R¬≤ score